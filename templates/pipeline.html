<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>üöÄ Process Pipeline</title>
  <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script>
</head>
<body>
  <nav class="main-nav">
    <a href="{{ url_for('main') }}" class="nav-link">üè† Home</a>
  </nav>
  <h1 class="title neon">Pose Classification Pipeline</h1>

  <section class="pipeline">
    <!-- Step 1 -->
    <div class="step" id="step-collect">
      <i class="fas fa-camera fa-3x"></i>
      <h2>1. Collect</h2>
      <p>Capture webcam poses via MediaPipe and save annotated landmarks to <code>data.csv</code>.</p>
      <div class="details">
        <h3>Under the Hood</h3>
        <ul>
          <li>MediaPipe Pose runs per-frame inference using a lightweight TFLite model.</li>
          <li>We select key joints (<code>LEFT_SHOULDER</code>, <code>RIGHT_ELBOW</code>, etc.).</li>
          <li>Coordinates (<code>x,y</code>) are normalized (0‚Äì1) and appended to <code>data.csv</code>.</li>
          <li>A 3-second countdown allows users to assume the correct pose.</li>
          <li>CSV header matches the landmark list for easy loading.</li>
        </ul>
      </div>
    </div>
    <div class="arrow" id="arrow1"><i class="fas fa-arrow-right fa-2x"></i></div>

    <!-- Step 2 -->
    <div class="step" id="step-train">
      <i class="fas fa-robot fa-3x"></i>
      <h2>2. Train</h2>
      <p>Engineer features, cross-validate, and export <code>pose_clf.pkl</code>.</p>
      <div class="details">
        <h3>Under the Hood</h3>
        <ul>
          <li><strong>Data Loading:</strong> <code>pandas</code> reads the CSV into a DataFrame.</li>
          <li><strong>Normalization:</strong> Each skeleton is centered at the hip midpoint and scaled by torso length, removing absolute size/position differences.</li>
          <li><strong>Geometric Features:</strong> Compute distances (e.g., wrist‚Äìelbow) and angles (e.g., shoulder‚Äìelbow‚Äìwrist) to capture intrinsic pose shape.</li>
          <li><strong>Temporal Deltas:</strong> For each frame, subtract the previous normalized skeleton to encode motion (Œîx, Œîy) as features.</li>
          <li><strong>Feature Vector:</strong> Concatenate [normalized coords, distances/angles, deltas] into a single input vector.</li>
          <li><strong>Stratified CV:</strong> Use <code>StratifiedKFold(n_splits=5)</code> to preserve class balance and compute fold accuracies.</li>
          <li><strong>Pipeline:</strong> <code>StandardScaler()</code> ‚Üí <code>RandomForestClassifier(n_estimators=100)</code> trained on each fold.</li>
          <li><strong>Final Model:</strong> Retrain on full dataset and serialize via <code>joblib.dump</code>.</li>
        </ul>
      </div>
    </div>
    <div class="arrow" id="arrow2"><i class="fas fa-arrow-right fa-2x"></i></div>

    <!-- Step 3 -->
    <div class="step" id="step-classify">
      <i class="fas fa-running fa-3x"></i>
      <h2>3. Classify</h2>
      <p>Real-time webcam inference, draw skeleton & confidence.</p>
      <div class="details">
        <h3>Under the Hood</h3>
        <ul>
          <li>Load <code>pose_clf.pkl</code> pipeline with <code>joblib.load</code>.</li>
          <li>Grab webcam frames; MediaPipe computes landmarks for the 12 key joints.</li>
          <li>Normalize and build [coords, geom, deltas] feature vector as during training.</li>
          <li>Predict with <code>predict</code> & <code>predict_proba</code> to get class and confidence.</li>
          <li>Draw the skeleton (no face/fingers) and overlay the predicted label in red.</li>
        </ul>
      </div>
    </div>
  </section>

  <script>
    document.addEventListener('DOMContentLoaded', () => {
      anime.timeline({ loop: false })
        .add({ targets:'#step-collect', opacity:[0,1], translateY:[20,0], duration:800 })
        .add({ targets:'#arrow1', opacity:[0,1], duration:600 })
        .add({ targets:'#step-train', opacity:[0,1], translateY:[20,0], duration:800 })
        .add({ targets:'#arrow2', opacity:[0,1], duration:600 })
        .add({ targets:'#step-classify', opacity:[0,1], translateY:[20,0], duration:800 });
    });

    document.querySelectorAll('.step').forEach(step => {
      step.addEventListener('click', () => {
        const d = step.querySelector('.details');
        const open = d.classList.toggle('open');
        anime({
          targets: d,
          height: open ? [0,d.scrollHeight] : [d.scrollHeight,0],
          opacity: open ? [0,1] : [1,0],
          easing: 'easeOutQuad',
          duration: 500
        });
      });
    });
  </script>
</body>
</html>
